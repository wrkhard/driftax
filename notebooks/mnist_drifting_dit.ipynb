{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# driftax MNIST \u2014 drifting DiT demo (fast)\n",
        "\n",
        "This notebook trains a small **DiT-like generator** on MNIST (28\u00d728\u00d71) using the drifting objective from:\n",
        "https://arxiv.org/abs/2602.04770\n",
        "\n",
        "What you\u2019ll see:\n",
        "- a few real digits\n",
        "- training loss curve\n",
        "- periodic grids of generated digits (conditioned on class label, 10\u00d710 grid)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "# Driftax notebook backend policy:\n",
        "# - macOS: FORCE CPU (avoid Metal instability)\n",
        "# - other OS: let JAX auto-select (GPU if available; else CPU)\n",
        "#\n",
        "# NOTE: must run BEFORE importing jax.\n",
        "for k in (\"JAX_PLATFORMS\", \"JAX_PLATFORM_NAME\"):\n",
        "    os.environ.pop(k, None)\n",
        "\n",
        "if sys.platform == \"darwin\":\n",
        "    os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If needed (from repo root), install:\n",
        "# !uv sync --extra dev\n",
        "# !uv run python -m pip install -e \".[dev]\"\n",
        "\n",
        "import os, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "from driftax.mnist import load_mnist_npz, preprocess_mnist\n",
        "from driftax.dit_latent2d import DiTLatent2D, DiTLatent2DConfig\n",
        "from driftax.conditioning import ClassEmbed\n",
        "from driftax.feature_encoders import TinyConvEncoder\n",
        "from driftax.drift import drifting_loss_features\n",
        "\n",
        "print(\"JAX devices:\", jax.devices())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load MNIST\n",
        "x_train_u8, y_train_u8, x_test_u8, y_test_u8 = load_mnist_npz(\"data\")\n",
        "x_train = preprocess_mnist(x_train_u8)  # float32 [-1,1], [N,28,28,1]\n",
        "y_train = y_train_u8.astype(np.int32)\n",
        "\n",
        "# Show a grid of real digits\n",
        "idx = np.random.default_rng(0).choice(len(x_train), size=100, replace=False)\n",
        "imgs = x_train[idx]\n",
        "\n",
        "def show_grid(imgs, nrow=10, title=\"\"):\n",
        "    imgs01 = (np.clip(imgs, -1, 1) + 1.0) * 0.5\n",
        "    N, H, W, C = imgs01.shape\n",
        "    ncol = int(np.ceil(N / nrow))\n",
        "    grid = np.ones((ncol * H, nrow * W), dtype=np.float32)\n",
        "    for i in range(N):\n",
        "        r = i // nrow\n",
        "        c = i % nrow\n",
        "        grid[r*H:(r+1)*H, c*W:(c+1)*W] = imgs01[i,:,:,0]\n",
        "    plt.figure(figsize=(nrow, ncol))\n",
        "    plt.imshow(grid, cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_grid(imgs, title=\"Real MNIST digits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build a small DiT for quick MNIST training\n",
        "cfg = DiTLatent2DConfig(\n",
        "    h=28, w=28, ch=1,\n",
        "    patch=2,\n",
        "    dim=256,\n",
        "    depth=6,\n",
        "    heads=8,\n",
        "    cond_dim=256,\n",
        "    num_context_tokens=0,\n",
        "    drop=0.0,\n",
        ")\n",
        "\n",
        "gen = DiTLatent2D(cfg)\n",
        "class_emb = ClassEmbed(num_classes=10, out_dim=cfg.cond_dim)\n",
        "\n",
        "# Feature encoder \u03d5 (fast placeholder; swap in stronger encoders later)\n",
        "phi = TinyConvEncoder(base=32)\n",
        "\n",
        "# Put training data on device\n",
        "x_train_d = jnp.asarray(x_train)\n",
        "y_train_d = jnp.asarray(y_train)\n",
        "N = x_train_d.shape[0]\n",
        "\n",
        "# Build per-class index table for fast conditional positives\n",
        "def make_class_index(labels_np, num_classes=10):\n",
        "    idxs = [np.where(labels_np == c)[0] for c in range(num_classes)]\n",
        "    counts = np.array([len(v) for v in idxs], dtype=np.int32)\n",
        "    M = int(max(counts))\n",
        "    table = np.zeros((num_classes, M), dtype=np.int32)\n",
        "    for c in range(num_classes):\n",
        "        v = idxs[c]\n",
        "        table[c, :len(v)] = v\n",
        "        table[c, len(v):] = v[0] if len(v) else 0\n",
        "    return table, counts\n",
        "\n",
        "class_table, class_counts = make_class_index(y_train, 10)\n",
        "class_table_d = jnp.asarray(class_table)\n",
        "class_counts_d = jnp.asarray(class_counts).astype(jnp.float32)\n",
        "\n",
        "def sample_pos_images(key, cls):\n",
        "    # cls: [B]\n",
        "    u = jax.random.uniform(key, cls.shape, dtype=jnp.float32)\n",
        "    cnt = jnp.take(class_counts_d, cls)  # [B]\n",
        "    r = jnp.minimum((u * cnt).astype(jnp.int32), (cnt.astype(jnp.int32) - 1))\n",
        "    idx = class_table_d[cls, r]\n",
        "    return x_train_d[idx]\n",
        "\n",
        "# init params\n",
        "key = jax.random.PRNGKey(0)\n",
        "key, k0, k1, k2 = jax.random.split(key, 4)\n",
        "dummy_z = jnp.zeros((1, cfg.h, cfg.w, cfg.ch), dtype=jnp.float32)\n",
        "dummy_cls = jnp.zeros((1,), dtype=jnp.int32)\n",
        "\n",
        "params = {}\n",
        "params[\"class_emb\"] = class_emb.init(k0, dummy_cls)\n",
        "cond0 = class_emb.apply(params[\"class_emb\"], dummy_cls)\n",
        "params[\"gen\"] = gen.init(k1, dummy_z, cond0, train=True)\n",
        "params[\"phi\"] = phi.init(k2, jnp.zeros((1, 28, 28, 1), dtype=jnp.float32), train=False)\n",
        "\n",
        "# optimizer\n",
        "lr = 2e-4\n",
        "opt = optax.adamw(lr)\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "temps = (0.02, 0.05, 0.2)\n",
        "batch = 128\n",
        "steps = 2000\n",
        "plot_every = 200\n",
        "\n",
        "def loss_fn(params, key):\n",
        "    key, kz, kc, kp = jax.random.split(key, 4)\n",
        "    cls = jax.random.randint(kc, (batch,), 0, 10)\n",
        "    cond = class_emb.apply(params[\"class_emb\"], cls)\n",
        "    z = jax.random.normal(kz, (batch, cfg.h, cfg.w, cfg.ch), dtype=jnp.float32)\n",
        "    x_gen = gen.apply(params[\"gen\"], z, cond, train=True)\n",
        "    x_pos = sample_pos_images(kp, cls)\n",
        "\n",
        "    fx_list = phi.apply(params[\"phi\"], x_gen, train=False)\n",
        "    fp_list = phi.apply(params[\"phi\"], x_pos, train=False)\n",
        "\n",
        "    # fast: global pooled features per scale\n",
        "    loss = 0.0\n",
        "    for fx, fp in zip(fx_list, fp_list):\n",
        "        fxv = jnp.mean(fx, axis=(1, 2))  # [B,C]\n",
        "        fpv = jnp.mean(fp, axis=(1, 2))\n",
        "        loss = loss + drifting_loss_features(\n",
        "            x_feat=fxv,\n",
        "            pos_feat=fpv,\n",
        "            temps=temps,\n",
        "            neg_feat=fxv,\n",
        "            feature_normalize=True,\n",
        "            drift_normalize=True,\n",
        "        )\n",
        "    return loss / float(len(fx_list))\n",
        "\n",
        "@jax.jit\n",
        "def step_fn(params, opt_state, key):\n",
        "    loss, grads = jax.value_and_grad(lambda p: loss_fn(p, key))(params)\n",
        "    updates, opt_state2 = opt.update(grads, opt_state, params)\n",
        "    params2 = optax.apply_updates(params, updates)\n",
        "    return params2, opt_state2, key, loss\n",
        "\n",
        "def sample_grid(params, key):\n",
        "    # 100 samples: 10 per class\n",
        "    cls = jnp.repeat(jnp.arange(10, dtype=jnp.int32), 10)\n",
        "    cond = class_emb.apply(params[\"class_emb\"], cls)\n",
        "    z = jax.random.normal(key, (cls.shape[0], cfg.h, cfg.w, cfg.ch), dtype=jnp.float32)\n",
        "    x = gen.apply(params[\"gen\"], z, cond, train=False)\n",
        "    return np.array(x)\n",
        "\n",
        "loss_hist = []\n",
        "for s in range(1, steps + 1):\n",
        "    params, opt_state, key, loss = step_fn(params, opt_state, key)\n",
        "    loss_hist.append(float(loss))\n",
        "\n",
        "    if s == 1 or (plot_every and s % plot_every == 0) or s == steps:\n",
        "        key, kvis = jax.random.split(key)\n",
        "        imgs = sample_grid(params, kvis)\n",
        "        show_grid(imgs, title=f\"Generated digits (step {s})\")\n",
        "\n",
        "# Loss curve\n",
        "loss_hist = np.asarray(loss_hist, dtype=np.float32)\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(loss_hist, alpha=0.8)\n",
        "plt.yscale(\"log\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
        "plt.title(\"driftax MNIST drifting loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}